{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los datos utilizando pandas\n",
    "datos=pd.read_csv(\"./data/data.csv\")\n",
    "\n",
    "# Creo la tabla de candidatos a mano\n",
    "candidatos=pd.DataFrame(\n",
    "    [\n",
    "        [1,'Oscar Andrade', 'Frente Amplio'],\n",
    "        [2,'Mario Bergara', 'Frente Amplio'],\n",
    "        [3,'Carolina Cosse', 'Frente Amplio'],\n",
    "        [4,'Daniel Martínez', 'Frente Amplio'],\n",
    "        [5,'Verónica Alonso', 'Partido Nacional'],\n",
    "        [6,'Enrique Antía', 'Partido Nacional'],\n",
    "        [8,'Carlos Iafigliola', 'Partido Nacional'],\n",
    "        [9,'Luis Lacalle Pou', 'Partido Nacional'],\n",
    "        [10,'Jorge Larrañaga', 'Partido Nacional'],\n",
    "        [11,'Juan Sartori', 'Partido Nacional'],\n",
    "        [12,'José Amorín', 'Partido Colorado'],\n",
    "        [13,'Pedro Etchegaray', 'Partido Colorado'],\n",
    "        [14,'Edgardo Martínez', 'Partido Colorado'],\n",
    "        [15,'Héctor Rovira', 'Partido Colorado'],\n",
    "        [16,'Julio María Sanguinetti', 'Partido Colorado'],\n",
    "        [17,'Ernesto Talvi', 'Partido Colorado'],\n",
    "        [18,'Pablo Mieres', 'La Alternativa'],\n",
    "        [19,'Gonzalo Abella', 'Unidad Popular'],        \n",
    "        [20,'Edgardo Novick', 'Partido de la Gente'],\n",
    "        [21,'Cèsar Vega', 'PERI'],\n",
    "        [22,'Rafael Fernández', 'Partido de los Trabajadores'],\n",
    "        [23,'Justin Graside', 'Partido Digital'],        \n",
    "        [24,'Gustavo Salle', 'Partido Verde'],\n",
    "        [25,'Carlos Techera', 'Partido de Todos']\n",
    "    ],\n",
    "    columns=['candidatoId','name','party'],\n",
    ")\n",
    "\n",
    "datos=datos.merge(candidatos,on=['candidatoId'])\n",
    "\n",
    "# Sólo por si necesita, cargamos un diccionario con el texto de cada pregunta\n",
    "preguntas={\n",
    "    '1': 'Controlar la inflación es más importante que controlar el desempleo. ',\n",
    "    '2': 'Hay que reducir la cantidad de funcionarios pùblicos',\n",
    "    '3': 'Deberia aumentar la carga de impuestos para los ricos.',\n",
    "    '4': 'El gobierno no debe proteger la industria nacional, si las fábricas no son competitivas esta bien que desaparezcan.',\n",
    "    '5': 'La ley de inclusión financiera es positiva para la sociedad. ',\n",
    "    '6': 'Algunos sindicatos tienen demasiado poder. ',\n",
    "    '7': 'Cuanto más libre es el mercado, más libre es la gente. ',\n",
    "    '8': 'El campo es y debe ser el motor productivo de Uruguay. ',\n",
    "    '9': 'La inversión extranjera es vital para que Uruguay alcance el desarrollo. ',\n",
    "    '10': 'Los supermercados abusan del pueblo con sus precios excesivos. ',\n",
    "    '11': 'Con la vigilancia gubernamental (escuchas telefonicas, e-mails y camaras de seguridad) el que no tiene nada que esconder, no tiene de que preocuparse. ',\n",
    "    '12': 'La pena de muerte debería ser una opción para los crímenes mas serios. ',\n",
    "    '13': 'Uruguay debería aprobar más leyes anti corrupción y ser más duro con los culpables. ',\n",
    "    '14': 'Las FF.AA. deberían tener un rol activo en la seguridad pública. ',\n",
    "    '15': 'Las carceles deberían ser administradas por organizaciones privadas. ',\n",
    "    '16': 'Hay que aumentar el salario de los policias significativamente. ',\n",
    "    '17': 'Para los delitos más graves hay que bajar la edad de imputabilidad a 16 años. ',\n",
    "    '18': 'Uruguay no necesita un ejército. ',\n",
    "    '19': 'Uruguay es demasiado generoso con los inmigrantes. ',\n",
    "    '20': 'La ley trans fue un error. ',\n",
    "    '21': 'El feminismo moderno no busca la igualdad sino el poder. ',\n",
    "    '22': 'Para la ley no deberia diferenciarse homicidio de femicidio. ',\n",
    "    '23': 'La separación de estado y religión me parece importante. ',\n",
    "    '24': 'La legalización de la marihuana fue un error. ',\n",
    "    '25': 'La legalización del aborto fue un error. ',\n",
    "    '26': 'El foco del próximo gobierno debe ser mejorar la educación pública. '\n",
    "}\n",
    "\n",
    "# Ordeno los datos por partido y luego por candidato\n",
    "\n",
    "datos = datos.sort_values(by=['party','name'])\n",
    "\n",
    "# eliminate candidates with less than 1000 votes\n",
    "counts_candidate = datos.name.value_counts().reset_index(name=\"count\").query(\"count > 1000\")\n",
    "counts_candidate.columns = [\"name\", \"count\"] \n",
    "filtered_data_candidate = datos.merge(counts_candidate, on=\"name\", how=\"inner\")\n",
    "\n",
    "# eliminate parties with less than 1000 votes\n",
    "counts_party =  datos.party.value_counts().reset_index(name=\"count\").query(\"count > 1000\")\n",
    "counts_party.columns = [\"party\", \"count\"] \n",
    "filtered_data_party = datos.merge(counts_party, on=\"party\", how=\"inner\")\n",
    "\n",
    "\n",
    "data_candidate = np.array(filtered_data_candidate)\n",
    "data_party = np.array(filtered_data_party)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creando cropus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data_party)\n",
    "cant_tuples = len(data_party)\n",
    "amount_training = round(cant_tuples*0.8)\n",
    "training_party, test_party = data_party[:amount_training,:], data_party[amount_training:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data_candidate)\n",
    "cant_tuples = len(data_candidate)\n",
    "amount_training = round(cant_tuples*0.8)\n",
    "training_candidate, test_candidate = data_candidate[:amount_training,:], data_candidate[amount_training:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalization_methods = ['l1', 'l2', 'elasticnet', 'none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING l1\n",
      "0.6564715414219161\n",
      "0.661535345351428\n",
      "0.6612641815235009\n",
      "0.6665316045380876\n",
      "0.6559967585089141\n",
      "\n",
      "USING l2\n",
      "0.6562689892647356\n",
      "0.6619404496657889\n",
      "0.6610615883306321\n",
      "0.6663290113452188\n",
      "0.6553889789303079\n",
      "\n",
      "USING elasticnet\n",
      "0.6562689892647356\n",
      "0.6619404496657889\n",
      "0.6610615883306321\n",
      "0.6663290113452188\n",
      "0.6553889789303079\n",
      "\n",
      "USING none\n",
      "0.6562689892647356\n",
      "0.6619404496657889\n",
      "0.6610615883306321\n",
      "0.6663290113452188\n",
      "0.6553889789303079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create classfier party\n",
    "\n",
    "for pen in penalization_methods:\n",
    "    if pen == 'elasticnet':\n",
    "        classifier_party = linear_model.LogisticRegression(penalty=pen, max_iter=10000, tol=0.000001, solver='saga', multi_class='multinomial', l1_ratio='0.5')\n",
    "    else:\n",
    "        classifier_party = linear_model.LogisticRegression(penalty=pen, max_iter=10000, tol=0.000001, solver='saga', multi_class='multinomial')\n",
    "    print(\"USING \" + pen)\n",
    "    # k fold cross validation training and evaluation\n",
    "    kf = KFold(n_splits = 5, shuffle=False)\n",
    "    for train_index, test_index in kf.split(training_party):\n",
    "        # get the current training and validation\n",
    "        party_train, party_test = data_party[train_index], data_party[test_index]\n",
    "        # strip the labels and extra metainfo\n",
    "        party_train_stripped = party_train[:,2:28]\n",
    "        party_test_stripped = party_test[:,2:28]\n",
    "        # get only the labels\n",
    "        party_train_labels = party_train[:,[30]]\n",
    "        party_test_labels = party_test[:,[30]]\n",
    "        # train\n",
    "        classifier_party.fit(party_train_stripped, party_train_labels.ravel())\n",
    "        # get the accuracy\n",
    "        print(classifier_party.score(party_test_stripped, party_test_labels.ravel()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l1', random_state=None, solver='saga',\n",
      "          tol=1e-06, verbose=0, warm_start=False)\n",
      "USING l1\n",
      "0.3940911367050576\n",
      "0.385411450509097\n",
      "0.38607911867801703\n",
      "0.3940911367050576\n",
      "0.38263772954924874\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=None, solver='saga',\n",
      "          tol=1e-06, verbose=0, warm_start=False)\n",
      "USING l2\n",
      "0.4021031547320981\n",
      "0.3875813720580871\n",
      "0.37973627107327657\n",
      "0.386246035720247\n",
      "0.38447412353923205\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='elasticnet', random_state=None, solver='saga',\n",
      "          tol=1e-06, verbose=0, warm_start=False)\n",
      "USING elasticnet\n",
      "0.3944249707895176\n",
      "0.3915873810716074\n",
      "0.38023702219996663\n",
      "0.39041896177599733\n",
      "0.38530884808013355\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=10000, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='none', random_state=None, solver='saga',\n",
      "          tol=1e-06, verbose=0, warm_start=False)\n",
      "USING none\n",
      "0.3939242196628276\n",
      "0.39025204473376734\n",
      "0.3827407778334168\n",
      "0.3915873810716074\n",
      "0.3876460767946578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create classfier candidate\n",
    "\n",
    "for pen in penalization_methods:\n",
    "    if pen == 'elasticnet':\n",
    "        classifier_candidate = linear_model.LogisticRegression(penalty=pen, max_iter=10000, tol=0.000001, solver='saga', multi_class='multinomial', l1_ratio='0.5') \n",
    "    else:\n",
    "        classifier_candidate = linear_model.LogisticRegression(penalty=pen, max_iter=10000, tol=0.000001, solver='saga', multi_class='multinomial')\n",
    "    print(\"USING \" + pen)\n",
    "    # k fold cross validation training and evaluation\n",
    "    kf = KFold(n_splits = 5, shuffle=False)\n",
    "    np.random.shuffle(data_candidate)\n",
    "    for train_index, test_index in kf.split(data_candidate):\n",
    "        # get the current training and validation\n",
    "        candidate_train, candidate_test = data_candidate[train_index], data_candidate[test_index]\n",
    "        # strip the labels and extra metainfo\n",
    "        candidate_train_stripped = candidate_train[:,2:28]\n",
    "        candidate_test_stripped = candidate_test[:,2:28]\n",
    "        # get only the labels\n",
    "        candidate_train_labels = candidate_train[:,[29]]\n",
    "        candidate_test_labels = candidate_test[:,[29]]\n",
    "        # train\n",
    "        classifier_candidate.fit(candidate_train_stripped, candidate_train_labels.ravel())\n",
    "        # get the accuracy\n",
    "        print(classifier_candidate.score(candidate_test_stripped, candidate_test_labels.ravel()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de componentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_accum_dict = {'Frente Amplio': 0, 'Partido Nacional': 0, 'Partido Colorado': 0, 'La Alternativa': 0, 'Unidad Popular': 0, 'Partido de la Gente': 0, 'PERI': 0, 'Partido de los Trabajadores': 0, 'Partido Digital': 0, 'Partido Verde': 0, 'Partido de Todos': 0}\n",
    "party_color_dict = {'Frente Amplio': '#15b01a', 'Partido Nacional': '#0343df', 'Partido Colorado': '#e50000', 'La Alternativa': '#029386', 'Unidad Popular': '#f97306', 'Partido de la Gente': '#ffff15', 'PERI': '#033500', 'Partido de los Trabajadores': '#ceb301', 'Partido Digital': '#0cff0c', 'Partido Verde': '#fe01b1', 'Partido de Todos': '#0485d1'}\n",
    "for key in party_accum_dict:\n",
    "    party_accum_dict[key] = len(datos[datos.party == key])\n",
    "sorted_accum_dict_keys = sorted(party_accum_dict)\n",
    "for i in range(1, len(sorted_accum_dict_keys)):\n",
    "    party_accum_dict[sorted_accum_dict_keys[i]] += party_accum_dict[sorted_accum_dict_keys[i - 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuestas = datos[[str(i) for i in range(1,27)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_respuestas = np.matrix(respuestas)\n",
    "matriz_respuestas = matriz_respuestas.T\n",
    "\n",
    "# Las filas de la matriz son los atributos. Las instancias están por columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de covarianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_medio = np.mean(matriz_respuestas, axis=1)\n",
    "matriz_respuestas_rescalada = matriz_respuestas - valor_medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_covarianza = np.cov(matriz_respuestas_rescalada)\n",
    "valores_propios, vectores_propios = np.linalg.eig(matriz_covarianza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_pair = [(np.abs(valores_propios[i]), vectores_propios[:,i], i) for i in range(len(valores_propios))]\n",
    "eigen_pair.sort()\n",
    "eigen_pair.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n is the amount of components\n",
    "n = 6\n",
    "matriz_w = np.hstack([(eigen_pair[i][1].reshape(26,1)) for i in range(0,n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = np.dot(matriz_respuestas_rescalada.T, matriz_w).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data_candidate)\n",
    "cant_tuples = len(data_candidate)\n",
    "amount_training = round(cant_tuples*0.8)\n",
    "training_candidate, test_candidate = data_candidate[:amount_training,:], data_candidate[amount_training:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING n = 1:\n",
      "0.21468808679324014\n",
      "0.21281034842478613\n",
      "0.20905487168787815\n",
      "0.20909849749582637\n",
      "0.2080550918196995\n",
      "\n",
      "USING n = 2:\n",
      "0.28958898393490506\n",
      "0.2658042979344878\n",
      "0.2712288754433549\n",
      "0.27295492487479134\n",
      "0.2773372287145242\n",
      "\n",
      "USING n = 3:\n",
      "0.3131650323388275\n",
      "0.29188399749634886\n",
      "0.3073231796369706\n",
      "0.3013355592654424\n",
      "0.3161519198664441\n",
      "\n",
      "USING n = 4:\n",
      "0.3137909451283121\n",
      "0.30398497809305236\n",
      "0.3131650323388275\n",
      "0.31030884808013354\n",
      "0.31176961602671116\n",
      "\n",
      "USING n = 5:\n",
      "0.3336115167953265\n",
      "0.31295639474233256\n",
      "0.32735238890047985\n",
      "0.3290901502504174\n",
      "0.3313856427378965\n",
      "\n",
      "USING n = 6:\n",
      "0.34174838305862715\n",
      "0.3250573753390361\n",
      "0.3379929063217192\n",
      "0.3332637729549249\n",
      "0.34286310517529217\n",
      "\n",
      "USING n = 7:\n",
      "0.34091383267264763\n",
      "0.32568328812852076\n",
      "0.3388274567076987\n",
      "0.33931552587646074\n",
      "0.3482888146911519\n",
      "\n",
      "USING n = 8:\n",
      "0.3402879198831629\n",
      "0.33110786563738787\n",
      "0.34759023576048403\n",
      "0.34202838063439067\n",
      "0.3434891485809683\n",
      "\n",
      "USING n = 9:\n",
      "0.34007928228666806\n"
     ]
    }
   ],
   "source": [
    "#Candidate\n",
    "\n",
    "# Obtain corpus partitions, note this gets all columns\n",
    "np.random.shuffle(data_candidate)\n",
    "cant_tuples = len(data_candidate)\n",
    "amount_training = round(cant_tuples*0.8)\n",
    "training_candidate, test_candidate = data_candidate[:amount_training,:], data_candidate[amount_training:,:]\n",
    "\n",
    "# create classifier\n",
    "classifier_candidate = linear_model.LogisticRegression(penalty='l1', max_iter=10000, tol=0.000001, solver='saga', multi_class='multinomial')\n",
    "\n",
    "# n is the amount of columns for PCA\n",
    "for n in range(1, 27):\n",
    "    matriz_w = np.hstack([(eigen_pair[i][1].reshape(26,1)) for i in range(0,n)])\n",
    "\n",
    "    transformed = np.dot(matriz_respuestas_rescalada.T, matriz_w).T\n",
    "\n",
    "    # Create 'filter' array of desired columns\n",
    "    principal_indices = [eigen_pair[i][2] for i in range(n)]\n",
    "    indices = np.array([j in principal_indices for j in range(26)])\n",
    "    indices = np.append([False,False],indices)\n",
    "    indices = np.append(indices,np.array([False,True,False,False]))\n",
    "    \n",
    "    # Apply filter to data\n",
    "    filtered_columns_training_candidate = training_candidate[:,indices]\n",
    "    filtered_columns_testing_candidate = test_candidate[:,indices]\n",
    "    \n",
    "    print('USING n = ' + str(n) + ':')\n",
    "    # k fold crossvalidation\n",
    "    kf = KFold(n_splits = 5, shuffle=False)\n",
    "    for train_index, test_index in kf.split(filtered_columns_training_candidate):\n",
    "        # get the current training and validation\n",
    "        candidate_train, candidate_test = filtered_columns_training_candidate[train_index], filtered_columns_training_candidate[test_index]\n",
    "        \n",
    "        # Strip the labels\n",
    "        candidate_train_stripped = candidate_train[:,0:n]\n",
    "        candidate_test_stripped = candidate_test[:,0:n]\n",
    "\n",
    "        # Obtain the labels\n",
    "        candidate_train_labels = candidate_train[:,[n]]\n",
    "        candidate_test_labels = candidate_test[:,[n]]\n",
    "    \n",
    "        #Train the classifier\n",
    "        classifier_candidate.fit(candidate_train_stripped, candidate_train_labels.ravel())\n",
    "\n",
    "        #Evaluate\n",
    "        \n",
    "        print(classifier_candidate.score(candidate_test_stripped, candidate_test_labels.ravel()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Party\n",
    "\n",
    "# Obtain corpus partitions, note this gets all columns\n",
    "np.random.shuffle(data_party)\n",
    "cant_tuples = len(data_party)\n",
    "amount_training = round(cant_tuples*0.8)\n",
    "training_party, test_party = data_party[:amount_training,:], data_party[amount_training:,:]\n",
    "\n",
    "# create classifier\n",
    "classifier_party = linear_model.LogisticRegression(penalty='l1', max_iter=1000, tol=0.00001, solver='saga', multi_class='multinomial')\n",
    "\n",
    "# n is the amount of columns for PCA\n",
    "for n in range(1, 27):\n",
    "    matriz_w = np.hstack([(eigen_pair[i][1].reshape(26,1)) for i in range(0,n)])\n",
    "\n",
    "    transformed = np.dot(matriz_respuestas_rescalada.T, matriz_w).T\n",
    "\n",
    "    # Create 'filter' array of desired columns\n",
    "    principal_indices = [eigen_pair[i][2] for i in range(n)]\n",
    "    indices = np.array([j in principal_indices for j in range(26)])\n",
    "    indices = np.append([False,False],indices)\n",
    "    indices = np.append(indices,np.array([False,False,True,False]))\n",
    "    \n",
    "    # Apply filter to data\n",
    "    filtered_columns_training_party = training_party[:,indices]\n",
    "    filtered_columns_testing_party = test_party[:,indices]\n",
    "    \n",
    "    print('USING n = ' + str(n) + ':')\n",
    "    # k fold crossvalidation\n",
    "    kf = KFold(n_splits = 5, shuffle=False)\n",
    "    for train_index, test_index in kf.split(filtered_columns_training_party):\n",
    "        # get the current training and validation\n",
    "        party_train, party_test = filtered_columns_training_party[train_index], filtered_columns_training_party[test_index]\n",
    "        \n",
    "        # Strip the labels\n",
    "        party_train_stripped = party_train[:,0:n]\n",
    "        party_test_stripped = party_test[:,0:n]\n",
    "\n",
    "        # Obtain the labels\n",
    "        party_train_labels = party_train[:,[n]]\n",
    "        party_test_labels = party_test[:,[n]]\n",
    "    \n",
    "        #Train the classifier\n",
    "        classifier_party.fit(party_train_stripped, party_train_labels.ravel())\n",
    "\n",
    "        #Evaluate\n",
    "        print(classifier_party.score(party_test_stripped, party_test_labels.ravel()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON PCA PARTY.\n",
    "# Obtain corpus partitions, note this gets all columns\n",
    "np.random.shuffle(data_party)\n",
    "cant_tuples = len(data_party)\n",
    "amount_training = round(cant_tuples*0.8)\n",
    "training_party, test_party = data_party[:amount_training,:], data_party[amount_training:,:]\n",
    "classifier_party = linear_model.LogisticRegression(penalty='l1', max_iter=1000, tol=0.00001, solver='saga', multi_class='multinomial')\n",
    "\n",
    "# Create 'filter' array of desired columns\n",
    "indices = np.array([True for _ in range(26)])\n",
    "indices = np.append([False,False],indices)\n",
    "indices = np.append(indices,np.array([False,False,True,False]))\n",
    "\n",
    "# Apply filter to data\n",
    "filtered_columns_training_party = training_party[:,indices]\n",
    "filtered_columns_testing_party = test_party[:,indices]\n",
    "\n",
    "party_train_stripped = filtered_columns_training_party[:,0:26]\n",
    "party_test_stripped = filtered_columns_testing_party[:,0:26]\n",
    "\n",
    "# Obtain the labels\n",
    "party_train_labels = filtered_columns_training_party[:,[26]]\n",
    "party_test_labels = filtered_columns_testing_party[:,[26]]\n",
    "#Train the classifier\n",
    "classifier_party.fit(party_train_stripped, party_train_labels.ravel())\n",
    "\n",
    "#Get labels non repeated:\n",
    "res=[]\n",
    "for elem in party_test_labels:\n",
    "    res.append(elem[0])\n",
    "print(list(set(res)))\n",
    "#Evaluate\n",
    "prediction_labels = classifier_party.predict(party_test_stripped)\n",
    "accuracy = metrics.accuracy_score(party_test_labels, prediction_labels)\n",
    "precision = metrics.precision_score(party_test_labels, prediction_labels,average='macro')\n",
    "recall = metrics.recall_score(party_test_labels, prediction_labels,average='macro')\n",
    "f_score = metrics.f1_score(party_test_labels, prediction_labels,average='macro')\n",
    "confusion_matrix = metrics.confusion_matrix(party_test_labels, prediction_labels, labels=list(set(res)))\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f_score)\n",
    "print(confusion_matrix)\n",
    "\n",
    "#La precision tira warning porque la regresión tiene clases que no clasificó, esto se detalla en el informe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA n = 25 PARTY.\n",
    "\n",
    "classifier_party = linear_model.LogisticRegression(penalty='l1', max_iter=1000, tol=0.00001, solver='saga', multi_class='multinomial')\n",
    "\n",
    "# Create 'filter' array of desired columns\n",
    "principal_indices = [eigen_pair[i][2] for i in range(25)]\n",
    "indices = np.array([j in principal_indices for j in range(26)])\n",
    "indices = np.append([False,False],indices)\n",
    "indices = np.append(indices,np.array([False,False,True,False]))\n",
    "\n",
    "# Apply filter to data\n",
    "filtered_columns_training_party = training_party[:,indices]\n",
    "filtered_columns_testing_party = test_party[:,indices]\n",
    "\n",
    "print('evaluating n = 25')\n",
    "\n",
    "party_train_stripped = filtered_columns_training_party[:,0:25]\n",
    "party_test_stripped = filtered_columns_testing_party[:,0:25]\n",
    "\n",
    "# Obtain the labels\n",
    "party_train_labels = filtered_columns_training_party[:,[25]]\n",
    "party_test_labels = filtered_columns_testing_party[:,[25]]\n",
    "#Train the classifier\n",
    "classifier_party.fit(party_train_stripped, party_train_labels.ravel())\n",
    "\n",
    "#Get labels non repeated:\n",
    "res=[]\n",
    "for elem in party_test_labels:\n",
    "    res.append(elem[0])\n",
    "print(list(set(res)))\n",
    "#Evaluate\n",
    "prediction_labels = classifier_party.predict(party_test_stripped)\n",
    "accuracy = metrics.accuracy_score(party_test_labels, prediction_labels)\n",
    "precision = metrics.precision_score(party_test_labels, prediction_labels,average='macro')\n",
    "recall = metrics.recall_score(party_test_labels, prediction_labels,average='macro')\n",
    "f_score = metrics.f1_score(party_test_labels, prediction_labels,average='macro')\n",
    "confusion_matrix = metrics.confusion_matrix(party_test_labels, prediction_labels, labels=list(set(res)))\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f_score)\n",
    "print(confusion_matrix)\n",
    "\n",
    "#La precision tira warning porque la regresión tiene clases que no clasificó, esto se detalla en el informe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON PCA CANDIDATE.\n",
    "# Obtain corpus partitions, note this gets all columns\n",
    "np.random.shuffle(data_candidate)\n",
    "cant_tuples = len(data_candidate)\n",
    "amount_training = round(cant_tuples*0.8)\n",
    "training_candidate, test_candidate = data_candidate[:amount_training,:], data_candidate[amount_training:,:]\n",
    "classifier_candidate = linear_model.LogisticRegression(penalty='l1', max_iter=1000, tol=0.00001, solver='saga', multi_class='multinomial')\n",
    "\n",
    "# Create 'filter' array of desired columns\n",
    "indices = np.array([True for _ in range(26)])\n",
    "indices = np.append([False,False],indices)\n",
    "indices = np.append(indices,np.array([False,True,False,False]))\n",
    "\n",
    "# Apply filter to data\n",
    "filtered_columns_training_candidate = training_candidate[:,indices]\n",
    "filtered_columns_testing_candidate = test_candidate[:,indices]\n",
    "\n",
    "party_train_stripped = filtered_columns_training_candidate[:,0:26]\n",
    "party_test_stripped = filtered_columns_testing_candidate[:,0:26]\n",
    "\n",
    "# Obtain the labels\n",
    "party_train_labels = filtered_columns_training_candidate[:,[26]]\n",
    "party_test_labels = filtered_columns_testing_candidate[:,[26]]\n",
    "#Train the classifier\n",
    "classifier_candidate.fit(party_train_stripped, party_train_labels.ravel())\n",
    "\n",
    "#Get labels non repeated:\n",
    "res=[]\n",
    "for elem in candidate_test_labels:\n",
    "    res.append(elem[0])\n",
    "print(list(set(res)))\n",
    "#Evaluate\n",
    "prediction_labels = classifier_candidate.predict(party_test_stripped)\n",
    "accuracy = metrics.accuracy_score(party_test_labels, prediction_labels)\n",
    "precision = metrics.precision_score(party_test_labels, prediction_labels,average='macro')\n",
    "recall = metrics.recall_score(party_test_labels, prediction_labels,average='macro')\n",
    "f_score = metrics.f1_score(party_test_labels, prediction_labels,average='macro')\n",
    "confusion_matrix = metrics.confusion_matrix(party_test_labels, prediction_labels, labels=list(set(res)))\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f_score)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA n=25 CANDIDATE.\n",
    "# Obtain corpus partitions, note this gets all columns\n",
    "np.random.shuffle(data_candidate)\n",
    "cant_tuples = len(data_candidate)\n",
    "amount_training = round(cant_tuples*0.8)\n",
    "training_candidate, test_candidate = data_candidate[:amount_training,:], data_candidate[amount_training:,:]\n",
    "classifier_candidate = linear_model.LogisticRegression(penalty='l1', max_iter=1000, tol=0.00001, solver='saga', multi_class='multinomial')\n",
    "\n",
    "principal_indices = [eigen_pair[i][2] for i in range(25)]\n",
    "indices = np.array([j in principal_indices for j in range(26)])\n",
    "indices = np.append([False,False],indices)\n",
    "indices = np.append(indices,np.array([False,True,False,False]))\n",
    "\n",
    "# Apply filter to data\n",
    "filtered_columns_training_candidate = training_candidate[:,indices]\n",
    "filtered_columns_testing_candidate = test_candidate[:,indices]\n",
    "\n",
    "party_train_stripped = filtered_columns_training_candidate[:,0:25]\n",
    "party_test_stripped = filtered_columns_testing_candidate[:,0:25]\n",
    "\n",
    "# Obtain the labels\n",
    "party_train_labels = filtered_columns_training_candidate[:,[25]]\n",
    "party_test_labels = filtered_columns_testing_candidate[:,[25]]\n",
    "#Train the classifier\n",
    "classifier_candidate.fit(party_train_stripped, party_train_labels.ravel())\n",
    "\n",
    "#Get labels non repeated:\n",
    "res=[]\n",
    "for elem in candidate_test_labels:\n",
    "    res.append(elem[0])\n",
    "print(list(set(res)))\n",
    "#Evaluate\n",
    "prediction_labels = classifier_candidate.predict(party_test_stripped)\n",
    "accuracy = metrics.accuracy_score(party_test_labels, prediction_labels)\n",
    "precision = metrics.precision_score(party_test_labels, prediction_labels,average='macro')\n",
    "recall = metrics.recall_score(party_test_labels, prediction_labels,average='macro')\n",
    "f_score = metrics.f1_score(party_test_labels, prediction_labels,average='macro')\n",
    "confusion_matrix = metrics.confusion_matrix(party_test_labels, prediction_labels, labels=list(set(res)))\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f_score)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificar por partido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar mejor clasificador de partidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtener mejor clasificador de candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain corpus partitions, note this gets all columns\n",
    "np.random.shuffle(data_candidate)\n",
    "cant_tuples = len(data_candidate)\n",
    "amount_training = round(cant_tuples*0.8)\n",
    "training_candidate, test_candidate = data_candidate[:amount_training,:], data_candidate[amount_training:,:]\n",
    "\n",
    "# Create classifier\n",
    "classifier_candidate = linear_model.LogisticRegression(penalty='l1', max_iter=10000, tol=0.000001, solver='saga', multi_class='multinomial')\n",
    "\n",
    "# Only use the n best columns according to PCA\n",
    "PCA = True\n",
    "if PCA:\n",
    "    best_n = 25\n",
    "else:\n",
    "    best_n = 26\n",
    "\n",
    "matriz_w = np.hstack([(eigen_pair[i][1].reshape(26,1)) for i in range(0,best_n)])\n",
    "\n",
    "transformed = np.dot(matriz_respuestas_rescalada.T, matriz_w).T\n",
    "\n",
    "# Create 'filter' array of desired columns\n",
    "principal_indices = [eigen_pair[i][2] for i in range(best_n)]\n",
    "indices = np.array([j in principal_indices for j in range(26)])\n",
    "indices = np.append([False,False],indices)\n",
    "indices = np.append(indices,np.array([False,True,False,False]))\n",
    "\n",
    "# Apply filter to data\n",
    "filtered_columns_training_candidate = training_candidate[:,indices]\n",
    "filtered_columns_testing_candidate = test_candidate[:,indices]\n",
    "\n",
    "candidate_train, candidate_test = filtered_columns_training_candidate[train_index], filtered_columns_training_candidate[test_index]\n",
    "\n",
    "# Strip the labels\n",
    "candidate_train_stripped = candidate_train[:,0:best_n]\n",
    "candidate_test_stripped = candidate_test[:,0:best_n]\n",
    "\n",
    "# Obtain the labels\n",
    "candidate_train_labels = candidate_train[:,[best_n]]\n",
    "candidate_test_labels = candidate_test[:,[best_n]]\n",
    "\n",
    "#Train the classifier\n",
    "classifier_candidate.fit(candidate_train_stripped, candidate_train_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the candidates dictionary\n",
    "candidates = filtered_data_candidate.copy()\n",
    "del candidates['candidatoId']\n",
    "candidates_dict = {}\n",
    "for elem in candidates.to_dict('records'):\n",
    "    candidates_dict[elem['name']] = elem['party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that returns the party from the candidate\n",
    "def classify_party(classified_candidate):\n",
    "    return candidates_dict[classified_candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_candidates_test = classifier_candidate.predict(candidate_test_stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_func = np.vectorize(classify_party)\n",
    "classified_entries = map_func(classified_candidates_test)\n",
    "print(classified_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the classifier for parties that uses the candidate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "prediction_labels = classified_entries\n",
    "party_test_labels = map_func(candidate_test_labels)\n",
    "labels = list(candidates_dict.values())\n",
    "labels = np.unique(labels)\n",
    "print(labels)\n",
    "accuracy = metrics.accuracy_score(party_test_labels, prediction_labels)\n",
    "precision = metrics.precision_score(party_test_labels, prediction_labels,average='macro')\n",
    "recall = metrics.recall_score(party_test_labels, prediction_labels,average='macro')\n",
    "f_score = metrics.f1_score(party_test_labels, prediction_labels,average='macro')\n",
    "confusion_matrix = metrics.confusion_matrix(party_test_labels, prediction_labels, labels=labels)\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f_score)\n",
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
